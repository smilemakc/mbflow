# AI-Powered Customer Support Workflow

Этот пример демонстрирует сложный workflow автоматизации клиентской поддержки с использованием искусственного интеллекта.

## Описание

Workflow выполняет следующие шаги:
1. **Извлечение информации о клиенте** - анализирует запрос и извлекает структурированные данные
2. **Классификация запроса** - определяет тип обращения (technical, billing, shipping, etc.)
3. **Анализ тональности** - определяет эмоциональную окраску сообщения (positive, neutral, negative)
4. **Маршрутизация** - направляет запрос в зависимости от типа:
   - Billing запросы → получение статуса аккаунта
   - Другие запросы → проверка необходимости эскалации
5. **Проверка эскалации** (OpenAI) - определяет, нужна ли помощь человека:
   - Технические проблемы с негативной тональностью
   - Высокая срочность
   - Требуется ручная проверка аккаунта
6. **Генерация ответа** - создает персонализированный ответ клиенту
7. **Контроль качества** - оценивает качество ответа
8. **Улучшение ответа** - регенерирует ответ при низком качестве
9. **Персонализация** - добавляет личные элементы в ответ
10. **Отправка и логирование** - отправляет ответ и записывает аналитику

## Изменения в этой версии

### ✅ Замена script-executor на openai-completion
Нода "Check Escalation Criteria" теперь использует OpenAI вместо JavaScript для принятия решений об эскалации. Это обеспечивает:
- Более гибкую логику принятия решений
- Лучшее понимание контекста
- Возможность учета нюансов в сообщениях клиентов

### ✅ Mock Server для API Endpoints
Добавлен встроенный mock-сервер (порт 8081), который имитирует следующие API:

#### 1. GET /accounts/{id}
Возвращает статус аккаунта клиента:
```json
{
  "account_id": "string",
  "status": "active|suspended|refund_eligible",
  "subscription_tier": "basic|premium|free",
  "balance": 120.50,
  "last_payment": "2025-11-20"
}
```

#### 2. POST /support/escalate
Создает тикет эскалации:
```json
{
  "ticket_id": "ESC-1234567890",
  "status": "escalated",
  "assigned_to": "senior_support_team",
  "priority": "high"
}
```

#### 3. POST /support/send
Отправляет ответ клиенту:
```json
{
  "message_id": "MSG-1234567890",
  "status": "sent",
  "delivery_status": "delivered"
}
```

#### 4. POST /analytics/log
Логирует взаимодействие:
```json
{
  "log_id": "LOG-1234567890",
  "status": "logged",
  "indexed": true
}
```

## Структура проекта

```
customer-support-ai/
├── main.go           # Основной workflow
├── mock_server.go    # Mock HTTP сервер
└── README.md         # Эта документация
```

## Запуск

1. Установите зависимости:
```bash
go mod download
```

2. Запустите пример:
```bash
go run .
```

Mock сервер автоматически запустится на порту 8081 и будет логировать все входящие запросы.

## Компиляция

Собрать исполняемый файл:
```bash
go build -o customer-support-demo .
```

Запустить скомпилированную версию:
```bash
./customer-support-demo
```

## Особенности

### Параллельная обработка
- Извлечение информации, классификация и анализ тональности выполняются параллельно
- Отправка ответа и генерация follow-up плана выполняются параллельно

### Условная маршрутизация
- Billing запросы → получение статуса аккаунта
- Проверка необходимости эскалации на основе AI анализа
- Разные пути обработки в зависимости от контекста

### Контроль качества
- Автоматическая оценка качества сгенерированного ответа
- Регенерация ответа при низком качестве (score < 7)
- Обратная связь для улучшения

### AI-powered Decision Making
- Все критические решения принимаются с помощью GPT-4
- Контекстно-зависимая логика эскалации
- Адаптивная генерация ответов

## Требования

- Go 1.21+
- OpenAI API ключ (для работы с LLM нодами)

## Примечания

- Mock сервер предоставляет реалистичные ответы для разработки и тестирования
- Все HTTP endpoints настроены на localhost:8081
- Логи mock сервера помогают отслеживать API вызовы в реальном времени
- В production окружении замените URL endpoints на реальные API сервисы
